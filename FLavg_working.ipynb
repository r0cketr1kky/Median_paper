{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FLavg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLNcEQcULdrHUZDNyzCUoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r0cketr1kky/Median_paper/blob/master/FLavg_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1yEaQAvU1Z3",
        "colab_type": "code",
        "outputId": "f53c7387-1c94-4114-ca5f-3d038992e520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/29/3fd78b6cecc540ebb31e676d269787d4bf5b4b90e479474a4f5ec9744bc0/syft-0.2.6-py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy~=1.4.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1)\n",
            "Collecting syft-proto~=0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/4c/90b80db263d05cc166e9940e9671b5e00cd019f5a9f7b390c99f63d1b918/syft_proto-0.4.8-py3-none-any.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hCollecting aiortc==0.9.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/54/beb8dcb9050aaf903c4426884970d0f18a76e49c69b5a06653812e404446/aiortc-0.9.28-cp36-cp36m-manylinux2010_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 12.0MB/s \n",
            "\u001b[?25hCollecting phe~=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n",
            "Collecting flask-socketio~=4.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting psutil==5.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack~=1.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.0)\n",
            "Collecting Pillow~=6.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/bbbc569f98f47813c50a116b539d97b3b17a86ac7a309f83b2022d26caf2/Pillow-6.2.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib~=1.6.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.6.0)\n",
            "Collecting torchvision~=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (from syft) (4.5.3)\n",
            "Collecting websocket-client~=0.57.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.2MB/s \n",
            "\u001b[?25hCollecting lz4~=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/81/011fef8766fb0ef681037ad6fee96168ee03a864464986cbaa23e5357704/lz4-3.0.2-cp36-cp36m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.18.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.18.5)\n",
            "Collecting websockets~=8.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.3MB/s \n",
            "\u001b[?25hCollecting torch~=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n",
            "\u001b[?25hCollecting requests~=2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask~=1.1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.2)\n",
            "Collecting protobuf>=3.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (1.14.0)\n",
            "Collecting aioice<0.7.0,>=0.6.17\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/86/e3cdf660b67da7a9a7013253db5db7cf786a52296cb40078db1206177698/aioice-0.6.18-py3-none-any.whl\n",
            "Collecting av<9.0.0,>=8.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/62/9a992be76f8e13ce0e3a24a838191b546805545116f9fc869bd11bd21b5f/av-8.0.2-cp36-cp36m-manylinux2010_x86_64.whl (36.9MB)\n",
            "\u001b[K     |████████████████████████████████| 36.9MB 88kB/s \n",
            "\u001b[?25hCollecting pylibsrtp>=0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/cc/89985f4c15320ed0d52b4074fd6f7baa6dd567d79414c10c9dbd9ade39b2/pylibsrtp-0.6.6-cp36-cp36m-manylinux2010_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
            "\u001b[?25hCollecting pyee>=6.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/28/1cedd44c27907f1507a28ff2d36fc6cdb981c9deff2fa288bc48a700c7c9/pyee-7.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiortc==0.9.28->syft) (0.7)\n",
            "Collecting crc32c\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/82/f60248c01a8a23ae07bd4c43d78d69b20ffe324311db3b0785e391aa09d2/crc32c-2.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting cryptography>=2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 43.2MB/s \n",
            "\u001b[?25hCollecting python-socketio>=4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/97/00741edd49788510b834b60a1a4d0afb2c4942770c11b8e0f6e914371718/python_socketio-4.6.0-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision~=0.5.0->syft) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests~=2.22.0->syft) (2020.4.5.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask~=1.1.1->syft) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.2->syft-proto~=0.4.5->syft) (47.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->aiortc==0.9.28->syft) (2.20)\n",
            "Collecting netifaces\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting python-engineio>=3.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/7e/d14f8ea2ad1aa7d28f8d95ffab8ec7a6034d81f7f52ef4d788c8e89cdcdc/python_engineio-3.13.0-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask~=1.1.1->syft) (1.1.1)\n",
            "Building wheels for collected packages: phe, psutil\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=18f5349569a7e00378660ec6b97037e3f31d8d86b857ab711461354a4580c2fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=272665 sha256=d09b7a87c5e901c0d03ba8ba7c8e6638d49d2dcd3bb75bd59a1dd5aebeae82e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/69/b4/3200b95828d1f0ddb3cb5699083717f4fdbd9b4223d0644c57\n",
            "Successfully built phe psutil\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: protobuf, syft-proto, netifaces, aioice, av, pylibsrtp, pyee, crc32c, cryptography, aiortc, phe, python-engineio, python-socketio, flask-socketio, psutil, Pillow, torch, torchvision, websocket-client, lz4, websockets, idna, requests, syft\n",
            "  Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "  Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "Successfully installed Pillow-6.2.2 aioice-0.6.18 aiortc-0.9.28 av-8.0.2 crc32c-2.0 cryptography-2.9.2 flask-socketio-4.2.1 idna-2.8 lz4-3.0.2 netifaces-0.10.9 phe-1.4.0 protobuf-3.12.2 psutil-5.7.0 pyee-7.0.2 pylibsrtp-0.6.6 python-engineio-3.13.0 python-socketio-4.6.0 requests-2.22.0 syft-0.2.6 syft-proto-0.4.8 torch-1.4.0 torchvision-0.5.0 websocket-client-0.57.0 websockets-8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "idna",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jCeZVudT-Lo",
        "colab_type": "code",
        "outputId": "d808ef59-269f-4eeb-8b37-3646ddd2ee08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTFS_euPUWI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class Parser:\n",
        "    \"\"\"Parameters for training\"\"\"\n",
        "    def __init__(self):\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.001\n",
        "        self.test_batch_size = 8\n",
        "        self.batch_size = 8\n",
        "        self.log_interval = 10\n",
        "        self.seed = 1\n",
        "    \n",
        "args = Parser()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "kwargs = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R8Ve961Ug1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/gdrive/My Drive/boston_housing.pickle','rb') as f:\n",
        "    #print(pickle.load(f))\n",
        "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
        "    #print(pickle.load(f))\n",
        "\n",
        "X = torch.from_numpy(X).float()\n",
        "y = torch.from_numpy(y).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "# preprocessing\n",
        "mean = X.mean(0, keepdim=True)\n",
        "dev = X.std(0, keepdim=True)\n",
        "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
        "dev[:, 3] = 1.  # so we don't standardize it\n",
        "X = (X - mean) / dev\n",
        "X_test = (X_test - mean) / dev\n",
        "train = TensorDataset(X, y)\n",
        "test = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_jj1sbUpgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(13, 32)\n",
        "        self.fc2 = nn.Linear(32, 24)\n",
        "        self.fc3 = nn.Linear(24, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 13)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeGuZDbXUvok",
        "colab_type": "code",
        "outputId": "6b750ea1-3c10-42df-f052-9de4cfaed751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import syft as sy\n",
        "\n",
        "hook = sy.TorchHook(torch)\n",
        "#bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "#alice = sy.VirtualWorker(hook, id=\"alice\")\n",
        "james = sy.VirtualWorker(hook, id=\"james\")\n",
        "\n",
        "x = [0] * 10\n",
        "for i in range(10):\n",
        "  x[i] = sy.VirtualWorker(hook, id=\"{}\".format(i))\n",
        "print(x)\n",
        "compute_nodes = x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<VirtualWorker id:0 #objects:0>, <VirtualWorker id:1 #objects:0>, <VirtualWorker id:2 #objects:0>, <VirtualWorker id:3 #objects:0>, <VirtualWorker id:4 #objects:0>, <VirtualWorker id:5 #objects:0>, <VirtualWorker id:6 #objects:0>, <VirtualWorker id:7 #objects:0>, <VirtualWorker id:8 #objects:0>, <VirtualWorker id:9 #objects:0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Lndec9Uz3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_distributed_dataset = []\n",
        "\n",
        "for batch_idx, (data,target) in enumerate(train_loader):\n",
        "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
        "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
        "    train_distributed_dataset.append((data, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvQj4BmgVgOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
        "        worker = data.location\n",
        "        model.send(worker)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # update the model\n",
        "        pred = model(data)\n",
        "        loss = F.mse_loss(pred.view(-1), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get()\n",
        "            \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get()\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3OU2JUVmUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ASmvBk2VpuN",
        "colab_type": "code",
        "outputId": "70642b71-b5bf-4465-ea6f-53288576c56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "t = time.time()\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(epoch)\n",
        "\n",
        "    \n",
        "total_time = time.time() - t\n",
        "print('Total', round(total_time, 2), 's')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/51 (0%)]\tLoss: 499.737000\n",
            "Train Epoch: 1 [80/51 (20%)]\tLoss: 444.433716\n",
            "Train Epoch: 1 [160/51 (39%)]\tLoss: 332.781464\n",
            "Train Epoch: 1 [240/51 (59%)]\tLoss: 133.071625\n",
            "Train Epoch: 1 [320/51 (78%)]\tLoss: 202.246475\n",
            "Train Epoch: 1 [200/51 (98%)]\tLoss: 16.212317\n",
            "Train Epoch: 2 [0/51 (0%)]\tLoss: 43.216484\n",
            "Train Epoch: 2 [80/51 (20%)]\tLoss: 8.341543\n",
            "Train Epoch: 2 [160/51 (39%)]\tLoss: 21.476643\n",
            "Train Epoch: 2 [240/51 (59%)]\tLoss: 24.701014\n",
            "Train Epoch: 2 [320/51 (78%)]\tLoss: 135.487076\n",
            "Train Epoch: 2 [200/51 (98%)]\tLoss: 16.858915\n",
            "Train Epoch: 3 [0/51 (0%)]\tLoss: 31.058374\n",
            "Train Epoch: 3 [80/51 (20%)]\tLoss: 6.284317\n",
            "Train Epoch: 3 [160/51 (39%)]\tLoss: 16.468430\n",
            "Train Epoch: 3 [240/51 (59%)]\tLoss: 20.231106\n",
            "Train Epoch: 3 [320/51 (78%)]\tLoss: 109.964317\n",
            "Train Epoch: 3 [200/51 (98%)]\tLoss: 14.596972\n",
            "Train Epoch: 4 [0/51 (0%)]\tLoss: 31.536037\n",
            "Train Epoch: 4 [80/51 (20%)]\tLoss: 6.582751\n",
            "Train Epoch: 4 [160/51 (39%)]\tLoss: 14.647831\n",
            "Train Epoch: 4 [240/51 (59%)]\tLoss: 15.352842\n",
            "Train Epoch: 4 [320/51 (78%)]\tLoss: 90.883629\n",
            "Train Epoch: 4 [200/51 (98%)]\tLoss: 13.207224\n",
            "Train Epoch: 5 [0/51 (0%)]\tLoss: 31.258949\n",
            "Train Epoch: 5 [80/51 (20%)]\tLoss: 7.210760\n",
            "Train Epoch: 5 [160/51 (39%)]\tLoss: 13.199809\n",
            "Train Epoch: 5 [240/51 (59%)]\tLoss: 11.943941\n",
            "Train Epoch: 5 [320/51 (78%)]\tLoss: 78.488693\n",
            "Train Epoch: 5 [200/51 (98%)]\tLoss: 12.271082\n",
            "Train Epoch: 6 [0/51 (0%)]\tLoss: 29.944611\n",
            "Train Epoch: 6 [80/51 (20%)]\tLoss: 7.750119\n",
            "Train Epoch: 6 [160/51 (39%)]\tLoss: 11.655148\n",
            "Train Epoch: 6 [240/51 (59%)]\tLoss: 10.470261\n",
            "Train Epoch: 6 [320/51 (78%)]\tLoss: 70.250114\n",
            "Train Epoch: 6 [200/51 (98%)]\tLoss: 11.836290\n",
            "Train Epoch: 7 [0/51 (0%)]\tLoss: 29.140284\n",
            "Train Epoch: 7 [80/51 (20%)]\tLoss: 7.879151\n",
            "Train Epoch: 7 [160/51 (39%)]\tLoss: 9.965445\n",
            "Train Epoch: 7 [240/51 (59%)]\tLoss: 9.868065\n",
            "Train Epoch: 7 [320/51 (78%)]\tLoss: 64.482712\n",
            "Train Epoch: 7 [200/51 (98%)]\tLoss: 11.616804\n",
            "Train Epoch: 8 [0/51 (0%)]\tLoss: 28.429167\n",
            "Train Epoch: 8 [80/51 (20%)]\tLoss: 8.152039\n",
            "Train Epoch: 8 [160/51 (39%)]\tLoss: 8.601878\n",
            "Train Epoch: 8 [240/51 (59%)]\tLoss: 9.669192\n",
            "Train Epoch: 8 [320/51 (78%)]\tLoss: 59.962429\n",
            "Train Epoch: 8 [200/51 (98%)]\tLoss: 11.715004\n",
            "Train Epoch: 9 [0/51 (0%)]\tLoss: 28.531357\n",
            "Train Epoch: 9 [80/51 (20%)]\tLoss: 8.310323\n",
            "Train Epoch: 9 [160/51 (39%)]\tLoss: 7.499834\n",
            "Train Epoch: 9 [240/51 (59%)]\tLoss: 9.452385\n",
            "Train Epoch: 9 [320/51 (78%)]\tLoss: 56.551575\n",
            "Train Epoch: 9 [200/51 (98%)]\tLoss: 11.322639\n",
            "Train Epoch: 10 [0/51 (0%)]\tLoss: 28.026104\n",
            "Train Epoch: 10 [80/51 (20%)]\tLoss: 8.640519\n",
            "Train Epoch: 10 [160/51 (39%)]\tLoss: 6.647422\n",
            "Train Epoch: 10 [240/51 (59%)]\tLoss: 9.245567\n",
            "Train Epoch: 10 [320/51 (78%)]\tLoss: 53.482048\n",
            "Train Epoch: 10 [200/51 (98%)]\tLoss: 10.839170\n",
            "Total 11.38 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-cveRV8VujB",
        "colab_type": "code",
        "outputId": "83c15af0-1b24-4437-af5f-30cd9e6d13bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 20.7677\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1TY-VS9V2GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENCRYPTED AGGREGATION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxY5OHvUZIJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3L8Mxv9V4AR",
        "colab_type": "code",
        "outputId": "04ab812d-699c-4a9e-fbd0-34f746474960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "remote_dataset = (list(),list(),list(),list(),list(),list(),list(),list(),list(),list())\n",
        "\n",
        "train_distributed_dataset = []\n",
        "print(compute_nodes)\n",
        "for batch_idx, (data,target) in enumerate(train_loader):\n",
        "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
        "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
        "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
        "\n",
        "def update(data, target, model, optimizer):\n",
        "    model.send(data.location)\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(data)\n",
        "    loss = F.mse_loss(pred.view(-1), target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return model\n",
        "\n",
        "#bobs_model = Net()\n",
        "#alices_model = Net()\n",
        "\n",
        "models = [0] * 10\n",
        "optimizers = [0] * 10\n",
        "params = [0] * 10\n",
        "for i in range(10):\n",
        "  models[i] = Net()\n",
        "  optimizers[i] = optim.SGD(models[i].parameters(), lr=args.lr)\n",
        "  params[i] = list(models[i].parameters())\n",
        "\n",
        "#print(len(params))\n",
        "#print(params[0])\n",
        "#bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
        "#alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
        "\n",
        "#models = [bobs_model, alices_model]\n",
        "#params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
        "#optimizers = [bobs_optimizer, alices_optimizer]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<VirtualWorker id:0 #objects:34>, <VirtualWorker id:1 #objects:8>, <VirtualWorker id:2 #objects:8>, <VirtualWorker id:3 #objects:8>, <VirtualWorker id:4 #objects:8>, <VirtualWorker id:5 #objects:8>, <VirtualWorker id:6 #objects:8>, <VirtualWorker id:7 #objects:8>, <VirtualWorker id:8 #objects:8>, <VirtualWorker id:9 #objects:11>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AULgLlPWA9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is selecting which batch to train on\n",
        "data_index = 0\n",
        "# update remote models\n",
        "# we could iterate this multiple times before proceeding, but we're only iterating once per worker here\n",
        "for remote_index in range(len(compute_nodes)):\n",
        "    data, target = remote_dataset[remote_index][data_index]\n",
        "    models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcIQwi94WD7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list where we'll deposit our encrypted model average\n",
        "new_params = list()\n",
        "# iterate through each parameter\n",
        "for param_i in range(len(params[0])):\n",
        "\n",
        "    # for each worker\n",
        "    spdz_params = list()\n",
        "    for remote_index in range(len(compute_nodes)):\n",
        "        \n",
        "        # since SMPC can only work with integers (not floats), we need\n",
        "        # to use Integers to store decimal information. In other words,\n",
        "        # we need to use \"Fixed Precision\" encoding.\n",
        "        fixed_precision_param = params[remote_index][param_i].fix_precision()\n",
        "        \n",
        "        # now we encrypt it on the remote machine. Note that \n",
        "        # fixed_precision_param is ALREADY a pointer. Thus, when\n",
        "        # we call share, it actually encrypts the data that the\n",
        "        # data is pointing TO. This returns a POINTER to the \n",
        "        # MPC secret shared object, which we need to fetch.\n",
        "        #print(fixed_precision_param)\n",
        "        #print(len(fixed_precision_param))\n",
        "\n",
        "        for i in range(9):\n",
        "          if i % 2 == 0:\n",
        "            \n",
        "            \n",
        "            encrypted_param = fixed_precision_param.share(x[i], x[i+1], crypto_provider=james)\n",
        "\n",
        "            #print(encrypted_param)\n",
        "        \n",
        "            # now we fetch the pointer to the MPC shared value\n",
        "            param = encrypted_param.get()\n",
        "        \n",
        "            # save the parameter so we can average it with the same parameter\n",
        "            # from the other workers\n",
        "            spdz_params.append(param)\n",
        "\n",
        "    # average params from multiple workers, fetch them to the local machine\n",
        "    # decrypt and decode (from fixed precision) back into a floating point number\n",
        "    #print(spdz_params)\n",
        "    \n",
        "    new_param = 0\n",
        "    for i in range(10):\n",
        "      new_param += (spdz_params[i]).get().float_precision()\n",
        "\n",
        "    new_param /= 10\n",
        "    #print(new_param)\n",
        "    # save the new averaged parameter\n",
        "    new_params.append(new_param)\n",
        "\n",
        "#print(new_params)\n",
        "with torch.no_grad():\n",
        "    for model in params:\n",
        "        for param in model:\n",
        "            param *= 0\n",
        "\n",
        "    for model in models:\n",
        "        model.get()\n",
        "\n",
        "    for remote_index in range(len(compute_nodes)):\n",
        "        for param_index in range(len(params[remote_index])):\n",
        "            params[remote_index][param_index].set_(new_params[param_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOcigHqWRlY",
        "colab_type": "code",
        "outputId": "40fad6ce-f63a-4b0a-c767-dc454ee1a78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "def train(epoch):\n",
        "    for data_index in range(len(remote_dataset[0])-1):\n",
        "        # update remote models\n",
        "        for remote_index in range(len(compute_nodes)):\n",
        "            data, target = remote_dataset[remote_index][data_index]\n",
        "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
        "\n",
        "        # encrypted aggregation\n",
        "        new_params = list()\n",
        "        for param_i in range(len(params[0])):\n",
        "            spdz_params = list()\n",
        "            for remote_index in range(len(compute_nodes)):\n",
        "                fixed_precision_param = params[remote_index][param_i].fix_precision()\n",
        "\n",
        "                for i in range(9):\n",
        "                  encrypted_param = fixed_precision_param.share(x[i], x[i+1], crypto_provider=james)\n",
        "\n",
        "                  param = encrypted_param.get()\n",
        "                \n",
        "                  spdz_params.append(param)\n",
        "\n",
        "                \n",
        "            new_param = 0\n",
        "            for i in range(10):\n",
        "              new_param += (spdz_params[i]).get().float_precision()\n",
        "            \n",
        "            new_param /= 10\n",
        "            # save the new averaged parameter\n",
        "            new_params.append(new_param)\n",
        "\n",
        "\n",
        "        # cleanup\n",
        "        with torch.no_grad():\n",
        "            for model in params:\n",
        "                for param in model:\n",
        "                    param *= 0\n",
        "\n",
        "            for model in models:\n",
        "                model.get()\n",
        "\n",
        "            for remote_index in range(len(compute_nodes)):\n",
        "                for param_index in range(len(params[remote_index])):\n",
        "                    params[remote_index][param_index].set_(new_params[param_index])\n",
        "def test():\n",
        "    models[0].eval()\n",
        "    test_loss = 0\n",
        "    for data, target in test_loader:\n",
        "        output = models[0](data)\n",
        "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))\n",
        "t = time.time()\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "    \n",
        "total_time = time.time() - t\n",
        "print('Total', round(total_time, 2), 's')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Test set: Average loss: 606.7265\n",
            "\n",
            "Epoch 2\n",
            "Test set: Average loss: 591.3130\n",
            "\n",
            "Epoch 3\n",
            "Test set: Average loss: 572.2477\n",
            "\n",
            "Epoch 4\n",
            "Test set: Average loss: 544.2882\n",
            "\n",
            "Epoch 5\n",
            "Test set: Average loss: 494.3641\n",
            "\n",
            "Epoch 6\n",
            "Test set: Average loss: 387.9724\n",
            "\n",
            "Epoch 7\n",
            "Test set: Average loss: 190.3325\n",
            "\n",
            "Epoch 8\n",
            "Test set: Average loss: 81.6172\n",
            "\n",
            "Epoch 9\n",
            "Test set: Average loss: 55.9037\n",
            "\n",
            "Epoch 10\n",
            "Test set: Average loss: 43.2665\n",
            "\n",
            "Total 77.29 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpF9hHhVWaCt",
        "colab_type": "code",
        "outputId": "5cbe2ccb-7818-429d-d1e8-87ca6ffef524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(test_loader.dataset))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkDVbAVkWnjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}